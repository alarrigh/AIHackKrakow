{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Prices Prediction\n",
    "\n",
    "In this tutorial we prepare a dataset with houses characteristics and sell prices and train a regression model for sales price prediction.\n",
    "\n",
    "The dataset used is the [Ames Housing Dataset](https://www.openintro.org/stat/data/?data=ames), which has variables describing (almost) every aspect of residential homes in Ames, Iowa.\n",
    "\n",
    "A detailed description of the variables in this dataset can be found [here](http://jse.amstat.org/v19n3/decock/DataDocumentation.txt)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the necessary packages and setting some notebook options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "\n",
    "from azureml.core import Experiment\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a [Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspaces) object, using the information from the configuration file that we uploaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = open('config/ws_config.json')\n",
    "cred_dict = json.load(config_file)\n",
    "\n",
    "ws = Workspace.from_config(path=\"./config/ws_config.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_history_name = 'my-run-history'\n",
    "\n",
    "# start a training run by defining an experiment\n",
    "experiment = Experiment(ws, \"01_house_prices_prediction\")\n",
    "run = experiment.start_logging()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we download the dataset described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"./data\"\n",
    "os.makedirs(data_folder, exist_ok = True)\n",
    "\n",
    "!wget https://www.openintro.org/stat/data/ames.csv -O ./data/ames.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset into a Pandas data frame, visualize the first 10 rows, and print the total number of rows and columns. We notice that this dataset has 2930 rows and 82 columns. Our response variable is the column named `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing = pd.read_csv(\"./data/ames.csv\")\n",
    "\n",
    "df_housing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We describe all columns and notice several things:\n",
    "  - the majority of the variables are categorical\n",
    "  - some categorical variables are wrongly encoded as numeric\n",
    "  - some numeric variables are wrongly encoded as categorical\n",
    "  - there are several missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_housing.describe(include = \"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop the `Order` and `PID` columns because they are unique identifiers and won't help predicting the house price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing[\"Order\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing[\"PID\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing.drop(\"Order\", axis = 1, inplace = True)\n",
    "df_housing.drop(\"PID\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now treat the missing values. To better analyze this, we create a function that builds a table with the missing percentage for each variable that has missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_missing_ratio(df):\n",
    "    df_housing_missing = (df.isnull().sum() / len(df)) * 100\n",
    "    df_housing_missing = df_housing_missing.drop(df_housing_missing[df_housing_missing == 0].index).sort_values(ascending = False)\n",
    "    display(pd.DataFrame({'Missing Ratio' :df_housing_missing}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_missing_ratio(df_housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we apply some strategies for imputing missing values, based on hints we gathered from the dataset description.\n",
    "\n",
    "For example, for some categorical variables a missing value represents a category like \"None\", and for some numerical variables it represents the value 0. For variables with relatively few missing values we can perform basic imputations like the median value for numeric variables and the mode value for categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_none = [\"Pool.QC\", \"Misc.Feature\", \"Alley\", \"Fence\", \"Fireplace.Qu\", \"Garage.Type\", \"Garage.Finish\", \"Garage.Qual\", \"Garage.Cond\",\n",
    "            \"Bsmt.Exposure\", \"Bsmt.Cond\", \"Bsmt.Qual\", \"Mas.Vnr.Type\"]\n",
    "for var in fill_none:\n",
    "    df_housing[var] = df_housing[var].fillna(\"None\")\n",
    "    \n",
    "fill_zero = [\"Garage.Yr.Blt\", \"BsmtFin.Type.2\", \"BsmtFin.Type.1\", \"Bsmt.Half.Bath\", \"Bsmt.Full.Bath\", \"Total.Bsmt.SF\",\n",
    "             \"Bsmt.Unf.SF\", \"BsmtFin.SF.1\", \"BsmtFin.SF.2\", \"Garage.Area\", \"Garage.Cars\", \"Mas.Vnr.Area\"]\n",
    "for var in fill_zero:\n",
    "    df_housing[var] = df_housing[var].fillna(0)\n",
    "\n",
    "df_housing[\"Lot.Frontage\"] = df_housing.groupby(\"Neighborhood\")[\"Lot.Frontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "df_housing['Electrical'] = df_housing['Electrical'].fillna(df_housing['Electrical'].mode()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_missing_ratio(df_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing[df_housing[\"Lot.Frontage\"].isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After imputing missing values for `Lot.Frontage` with the median values of `Lot.Frontage` by Neighborhood, we notice there are still missing values for that variable.\n",
    "\n",
    "This is because there is one neighborhood with only one house and its value for `Lot.Frontage` is missing. And there is another neighborhood with only two houses with both values for `Lot.Frontage` also missing.\n",
    "\n",
    "We discard those records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing = df_housing.dropna()\n",
    "\n",
    "compute_missing_ratio(df_housing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we correct some data types, according to our interpretation of continuous and categorical variables in this dataset. We represent numerical continuous values as float numbers and categorical as strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_housing.columns)\n",
    "run.log_list(\"columns\", df_housing.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_var = [\"SalePrice\"]\n",
    "\n",
    "numeric_vars = [\"Lot.Frontage\", \"Lot.Area\", \"Mas.Vnr.Area\", \"BsmtFin.SF.1\", \"BsmtFin.SF.2\", \"Bsmt.Unf.SF\", \"Total.Bsmt.SF\",\n",
    "                \"X1st.Flr.SF\", \"X2nd.Flr.SF\", \"Low.Qual.Fin.SF\", \"Gr.Liv.Area\", \"Garage.Area\", \"Wood.Deck.SF\",\n",
    "                \"Open.Porch.SF\", \"Enclosed.Porch\", \"X3Ssn.Porch\", \"Screen.Porch\", \"Pool.Area\", \"Misc.Val\"]\n",
    "\n",
    "categorical_vars = [v for v in df_housing.columns if v not in numeric_vars + response_var]\n",
    "\n",
    "df_housing[response_var] = df_housing[response_var].astype(float)\n",
    "df_housing[numeric_vars] = df_housing[numeric_vars].astype(float)\n",
    "df_housing[categorical_vars] = df_housing[categorical_vars].astype(str)\n",
    "\n",
    "display(pd.DataFrame({\"Data Type\" :df_housing.dtypes}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finishing the data cleaning, we then visualize relashionships between variables.\n",
    "\n",
    "We begin with scatterplots between `SalePrice` and other continuous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df_housing, y_vars = response_var, x_vars = numeric_vars[0:7])\n",
    "sns.pairplot(df_housing, y_vars = response_var, x_vars = numeric_vars[7:13])\n",
    "sns.pairplot(df_housing, y_vars = response_var, x_vars = numeric_vars[13:19])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create boxplots of `SalesPrice` according to the categories given by the categorical variables.\n",
    "\n",
    "To better visualize this, we first encode each categorical variable by ordering its distinct category values according to the mean of `SalePrice` calculated for each category value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(frame, feature):\n",
    "    ordering = pd.DataFrame()\n",
    "    ordering[\"val\"] = frame[feature].unique()\n",
    "    ordering.index = ordering.val\n",
    "    ordering[\"spmean\"] = frame[[feature, \"SalePrice\"]].groupby(feature).mean()[\"SalePrice\"]\n",
    "    ordering = ordering.sort_values(\"spmean\")\n",
    "    ordering[\"ordering\"] = range(1, ordering.shape[0]+1)\n",
    "    ordering = ordering[\"ordering\"].to_dict()\n",
    "    \n",
    "    for cat, o in ordering.items():\n",
    "        frame.loc[frame[feature] == cat, feature+'_E'] = o\n",
    "    \n",
    "categorical_vars_E = []\n",
    "for q in categorical_vars:  \n",
    "    encode(df_housing, q)\n",
    "    categorical_vars_E.append(q+\"_E\")\n",
    "\n",
    "def boxplot(x, y, **kwargs):\n",
    "    sns.boxplot(x = x, y = y)\n",
    "    x = plt.xticks(rotation = 90)\n",
    "\n",
    "data = pd.melt(df_housing, id_vars = [\"SalePrice\"], value_vars = categorical_vars_E)\n",
    "g = sns.FacetGrid(data, col = \"variable\",  col_wrap = 5, sharex = False, sharey = False)\n",
    "g = g.map(boxplot, \"value\", \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we plot the Spearman Correlation between `SalePrice` and each variable.\n",
    "\n",
    "For this to make sense for the categorical variables, we use the previous numeric ordered encoded values to represent each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spearman(frame, features):\n",
    "    spr = pd.DataFrame()\n",
    "    spr[\"feature\"] = features\n",
    "    spr[\"spearman\"] = [frame[f].corr(frame[\"SalePrice\"], \"spearman\") for f in features]\n",
    "    spr = spr.sort_values(\"spearman\")\n",
    "    plt.figure(figsize = (6, 0.25*len(features)))\n",
    "    ax = sns.barplot(data=spr, y=\"feature\", x = \"spearman\", orient = \"h\")\n",
    "    ax.set(title = \"Spearman Correlation\", ylabel = \"feature\", xlabel = \"spearman\")\n",
    "    \n",
    "features = numeric_vars + categorical_vars_E\n",
    "spearman(df_housing, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we visualize the distribution of `SalePrice`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(df_housing[response_var])\n",
    "ax.set(title = \"Distribution of SalePrice\", xlabel = \"SalePrice\", ylabel = \"frequency\")\n",
    "# plt.show()\n",
    "run.log_image(\"distplot\", path = None, plot = plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(df_housing[response_var])\n",
    "ax.set(title = \"Distribution of SalePrice\", xlabel = \"SalePrice\")\n",
    "# plt.show()\n",
    "run.log_image(\"boxplot\", path = None, plot = plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not going to perform any outlier analysis, feature selection or transformation here. Instead, we will try to model `SalePrice` directly using a non-linear regression algorithm.\n",
    "\n",
    "We use Gradient Boosting Regression as an example of an ML algorithm. The first step here is to split the dataset in a training portion and a test portion for final evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_housing[numeric_vars + categorical_vars_E], df_housing[response_var],\n",
    "                                                    test_size = 0.4, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a loop for model training with grid search for hyperparameter selection and using the training data for 5-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter_grid = [{'n_estimators': [250,500,1000], 'max_depth': [4,8], 'min_samples_split': [2,4],\n",
    "                   'learning_rate': [0.01], 'loss': ['ls']}]\n",
    "\n",
    "scores = {'R2': make_scorer(r2_score, greater_is_better = True), 'MAE': make_scorer(mean_absolute_error, greater_is_better = False)}\n",
    "\n",
    "cv_models = {}\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\\n\" % score)\n",
    "\n",
    "    clf = GridSearchCV(GradientBoostingRegressor(), parameter_grid, cv = 5, scoring = scores[score], n_jobs = -1)\n",
    "    clf.fit(X_train, y_train)\n",
    "    cv_models[score] = clf\n",
    "\n",
    "    print(\"Best parameters set found on development set:\\n\")\n",
    "    print(clf.best_params_)\n",
    "    print(\"Grid scores on development set:\\n\")\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\\n\" % (mean, std * 2, params))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.log_list(\"means\", means, description = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the best model and compute metrics for train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = cv_models['MAE'].best_estimator_\n",
    "\n",
    "def MAPE(y_actual, y_predict):\n",
    "    sum_actuals = sum_errors = 0\n",
    "    \n",
    "    for actual_val, predict_val in zip(y_actual, y_predict):\n",
    "        abs_error = actual_val - predict_val\n",
    "        if abs_error < 0:\n",
    "            abs_error = abs_error * -1\n",
    "\n",
    "        sum_errors = sum_errors + abs_error\n",
    "        sum_actuals = sum_actuals + actual_val\n",
    "\n",
    "    return sum_errors / sum_actuals\n",
    "\n",
    "y_pred_train = best_model.predict(X_train)\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "y_true_train = y_train.values.flatten()\n",
    "y_true_test = y_test.values.flatten()\n",
    "\n",
    "print(\"MAPE (Train): %f\" % MAPE(y_true_train, y_pred_train))\n",
    "print(\"MAPE (Test): %f\" % MAPE(y_true_test, y_pred_test))\n",
    "\n",
    "run.log(\"MAPE (Train)\", MAPE(y_true_train, y_pred_train))\n",
    "run.log(\"MAPE (Test)\", MAPE(y_true_test, y_pred_test))\n",
    "\n",
    "print(\"MAE (Train): %f\" % mean_absolute_error(y_true_train, y_pred_train))\n",
    "print(\"MAE (Test): %f\" % mean_absolute_error(y_true_test, y_pred_test))\n",
    "\n",
    "run.log(\"MAE (Train)\", mean_absolute_error(y_true_train, y_pred_train))\n",
    "run.log(\"MAE (Test)\", mean_absolute_error(y_true_test, y_pred_test))\n",
    "\n",
    "print(\"R2 (Train): %f\" % r2_score(y_true_train, y_pred_train))\n",
    "print(\"R2 (Test): %f\" % r2_score(y_true_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute relative feature importances using the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance = best_model.feature_importances_\n",
    "# make importances relative to max importance\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "var_names = np.asarray(numeric_vars + categorical_vars)\n",
    "fig = plt.figure(figsize=(12, 20))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align = 'center')\n",
    "plt.yticks(pos, var_names[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "# plt.show()\n",
    "run.log_image(\"variable_importance\", path = None, plot = plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now visualize the plots for predicted versus actual values for both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y = y_pred_train, x = y_true_train)\n",
    "plt.plot(y_true_train, y_true_train, color = \"red\")\n",
    "plt.title(\"Predicted vs Actuals (Train)\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y = y_pred_test, x = y_true_test)\n",
    "plt.plot(y_true_train, y_true_train, color = \"red\")\n",
    "plt.title(\"Predicted vs Actuals (Test)\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we plot the error distributions for both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(y_pred_train - y_true_train)\n",
    "ax.set(title = \"Distribution of Errors (Train)\", xlabel = \"SalePrice\", ylabel = \"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(y_pred_test - y_true_test)\n",
    "ax.set(title = \"Distribution of Errors (Test)\", xlabel = \"SalePrice\", ylabel = \"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to declare logging as complete so the run can be marked as complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run.complete()\n",
    "print (\"run id:\", run.id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
