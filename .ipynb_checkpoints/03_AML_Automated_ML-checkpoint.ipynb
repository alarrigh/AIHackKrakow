{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "\n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## House Prices Prediction\n",
    "### Model training with Automated Machine Learning and Model Explainability\n",
    "\n",
    "In this tutorial we will perform the same basic data preparation and model training steps as we did before, but know with the help of Azure ML service for Automated Machine Learning and Model Explainability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by importing the necessary packages and setting some notebook options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'azureml.train.automl.automl_explain_utilities'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-70a93902f4ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplanation_client\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mExplanationClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoml_explain_utilities\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoMLExplainerSetupClass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mautoml_setup_model_explanations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlightgbm_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMExplainableModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mazureml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmimic_wrapper\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMimicWrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'azureml.train.automl.automl_explain_utilities'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import make_scorer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os, logging\n",
    "import json\n",
    "\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "from azureml.explain.model._internal.explanation_client import ExplanationClient\n",
    "from azureml.train.automl.automl_explain_utilities import AutoMLExplainerSetupClass, automl_setup_model_explanations\n",
    "from azureml.explain.model.mimic.models.lightgbm_model import LGBMExplainableModel\n",
    "from azureml.explain.model.mimic_wrapper import MimicWrapper\n",
    "from azureml.contrib.explain.model.visualize import ExplanationDashboard\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to train locally. Therefore, instead of executing the data loading and preparation steps in a separate script, we just execute them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_housing = pd.read_csv(\"./data/ames.csv\")\n",
    "\n",
    "df_housing.drop(\"Order\", axis = 1, inplace = True)\n",
    "df_housing.drop(\"PID\", axis = 1, inplace = True)\n",
    "\n",
    "fill_none = [\"Pool.QC\", \"Misc.Feature\", \"Alley\", \"Fence\", \"Fireplace.Qu\", \"Garage.Type\", \"Garage.Finish\", \"Garage.Qual\", \"Garage.Cond\",\n",
    "            \"Bsmt.Exposure\", \"Bsmt.Cond\", \"Bsmt.Qual\", \"Mas.Vnr.Type\"]\n",
    "for var in fill_none:\n",
    "    df_housing[var] = df_housing[var].fillna(\"None\")\n",
    "    \n",
    "fill_zero = [\"Garage.Yr.Blt\", \"BsmtFin.Type.2\", \"BsmtFin.Type.1\", \"Bsmt.Half.Bath\", \"Bsmt.Full.Bath\", \"Total.Bsmt.SF\",\n",
    "             \"Bsmt.Unf.SF\", \"BsmtFin.SF.1\", \"BsmtFin.SF.2\", \"Garage.Area\", \"Garage.Cars\", \"Mas.Vnr.Area\"]\n",
    "for var in fill_zero:\n",
    "    df_housing[var] = df_housing[var].fillna(0)\n",
    "\n",
    "df_housing[\"Lot.Frontage\"] = df_housing.groupby(\"Neighborhood\")[\"Lot.Frontage\"].transform(lambda x: x.fillna(x.median()))\n",
    "\n",
    "df_housing['Electrical'] = df_housing['Electrical'].fillna(df_housing['Electrical'].mode()[0])\n",
    "\n",
    "df_housing = df_housing.dropna()\n",
    "\n",
    "response_var = [\"SalePrice\"]\n",
    "\n",
    "numeric_vars = [\"Lot.Frontage\", \"Lot.Area\", \"Mas.Vnr.Area\", \"BsmtFin.SF.1\", \"BsmtFin.SF.2\", \"Bsmt.Unf.SF\", \"Total.Bsmt.SF\",\n",
    "                \"X1st.Flr.SF\", \"X2nd.Flr.SF\", \"Low.Qual.Fin.SF\", \"Gr.Liv.Area\", \"Garage.Area\", \"Wood.Deck.SF\",\n",
    "                \"Open.Porch.SF\", \"Enclosed.Porch\", \"X3Ssn.Porch\", \"Screen.Porch\", \"Pool.Area\", \"Misc.Val\"]\n",
    "\n",
    "categorical_vars = [v for v in df_housing.columns if v not in numeric_vars + response_var]\n",
    "\n",
    "def encode(frame, feature):\n",
    "    ordering = pd.DataFrame()\n",
    "    ordering['val'] = frame[feature].unique()\n",
    "    ordering.index = ordering.val\n",
    "    ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n",
    "    ordering = ordering.sort_values('spmean')\n",
    "    ordering['ordering'] = range(1, ordering.shape[0]+1)\n",
    "    ordering = ordering['ordering'].to_dict()\n",
    "    \n",
    "    for cat, o in ordering.items():\n",
    "        frame.loc[frame[feature] == cat, feature + '_E'] = o\n",
    "    \n",
    "categorical_vars_E = []\n",
    "for q in categorical_vars:  \n",
    "    encode(df_housing, q)\n",
    "    df_housing[q + '_E'] = df_housing[q + '_E'].astype('int')\n",
    "    categorical_vars_E.append(q + '_E')\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(df_housing[numeric_vars + categorical_vars_E], df_housing[response_var],\n",
    "                                                    test_size = 0.4, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate a [Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#workspaces) object, using the information from the configuration file that we uploaded previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = open('config/ws_config.json')\n",
    "cred_dict = json.load(config_file)\n",
    "\n",
    "auth = ServicePrincipalAuthentication(tenant_id = cred_dict['tentant_id'], \n",
    "                                      service_principal_id = cred_dict['service_principal_id'], \n",
    "                                      service_principal_password = cred_dict['service_principal_password'])\n",
    "\n",
    "ws = Workspace.from_config(path = \"./config/ws_config.json\", auth = auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step before using [automated machine learning](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-automated-ml) is to define the configuration for the AutoML job submission.\n",
    "\n",
    "This is done by creating a [AutoMLConfig](https://docs.microsoft.com/en-us/python/api/azureml-train-automl/azureml.train.automl.automlconfig?view=azure-ml-py) object.\n",
    "\n",
    "There are several parameters in the [configuration](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train) that control the automated machine learning job execution, such as number of iterations, iteration timeout, primary model metric, and many others.\n",
    "\n",
    "Here we are defining some of those parameters in a dictionary, to be later unpacked as arguments when creating the AutoMLConfig object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    \"iteration_timeout_minutes\": 15,\n",
    "    \"iterations\": 15,\n",
    "    \"primary_metric\": \"normalized_mean_absolute_error\",\n",
    "    \"preprocess\": True,\n",
    "    \"verbosity\": logging.INFO,\n",
    "    \"n_cross_validations\": 5,\n",
    "    \"model_explainability\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task = \"regression\",\n",
    "                             debug_log = \"automated_ml_errors.log\",\n",
    "                             X = X_train,\n",
    "                             y = y_train,\n",
    "                             **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate an [Experiment](https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-azure-machine-learning-architecture#experiments) object and then submit our automated machine learning job for execution, passing the **AutoMLConfig** object created before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "experiment = Experiment(ws, \"03_AML_Automated_ML\")\n",
    "local_run = experiment.submit(automl_config, show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can monitor the execution through a Jupyter [graphical widget](https://docs.microsoft.com/en-us/azure/machine-learning/service/tutorial-auto-train-models#explore-the-results), available through the RunDetails class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RunDetails(local_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the job is completed, we can access the best run and best trained model.\n",
    "\n",
    "By printing the best trained model, we can see the [steps](https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train#understand-automated-ml-models) that were automatically executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = 'housing price model trained using AutoML'\n",
    "tags = None\n",
    "model = local_run.register_model(description = description, tags = tags, iteration = 3)\n",
    "local_run.model_id # Use this id to deploy the model as a web service in Azur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will refer to the model name returned above in our upcoming deployment notebook.\n",
    "\n",
    "Here we compute model performance metrics for train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def MAPE(y_actual, y_predict):\n",
    "    sum_actuals = sum_errors = 0\n",
    "    \n",
    "    for actual_val, predict_val in zip(y_actual, y_predict):\n",
    "        abs_error = actual_val - predict_val\n",
    "        if abs_error < 0:\n",
    "            abs_error = abs_error * -1\n",
    "\n",
    "        sum_errors = sum_errors + abs_error\n",
    "        sum_actuals = sum_actuals + actual_val\n",
    "\n",
    "    return sum_errors / sum_actuals\n",
    "\n",
    "y_pred_train = fitted_model.predict(X_train)\n",
    "y_pred_test = fitted_model.predict(X_test)\n",
    "y_true_train = y_train.values.flatten()\n",
    "y_true_test = y_test.values.flatten()\n",
    "\n",
    "print(\"MAPE (Train): %f\" % MAPE(y_true_train, y_pred_train))\n",
    "print(\"MAPE (Test): %f\" % MAPE(y_true_test, y_pred_test))\n",
    "\n",
    "print(\"Acc (Train): %f\" % (1 - MAPE(y_true_train, y_pred_train)))\n",
    "print(\"Acc (Test): %f\" % (1 - MAPE(y_true_test, y_pred_test)))\n",
    "\n",
    "print(\"MAE (Train): %f\" % mean_absolute_error(y_true_train, y_pred_train))\n",
    "print(\"MAE (Test): %f\" % mean_absolute_error(y_true_test, y_pred_test))\n",
    "\n",
    "print(\"R2 (Train): %f\" % r2_score(y_true_train, y_pred_train))\n",
    "print(\"R2 (Test): %f\" % r2_score(y_true_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the plots for predicted versus actual values for both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y = y_pred_train, x = y_true_train)\n",
    "plt.plot(y_true_train, y_true_train, color = \"red\")\n",
    "plt.title(\"Predicted vs Actuals (Train)\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(y = y_pred_test, x = y_true_test)\n",
    "plt.plot(y_true_test, y_true_test, color = \"red\")\n",
    "plt.title(\"Predicted vs Actuals (Test)\")\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's plot the error distributions for both train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(y_pred_train - y_true_train)\n",
    "ax.set(title = \"Distribution of Errors (Train)\", xlabel = \"SalePrice\", ylabel = \"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.distplot(y_pred_test - y_true_test)\n",
    "ax.set(title = \"Distribution of Errors (Test)\", xlabel = \"SalePrice\", ylabel = \"frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a final step, we are going to show how to use some of the [model interpretability](https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability) features available on AML service.\n",
    "\n",
    "Please notice that these are experimental features, and the supported functionality and API interfaces are still evolving.\n",
    "\n",
    "Here we instantiate a **ExplanationClient** object, passing the best run from the job execution, to get variable feature importance scores for the engineered features (after the transformations performed automatically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ExplanationClient.from_run(best_run)\n",
    "engineered_explanations = client.download_model_explanation(raw = False)\n",
    "feature_importance_dict = engineered_explanations.get_feature_importance_dict()\n",
    "{k:v for k,v in feature_importance_dict.items() if v > 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, for a categorical feature, an importance score is generated for each category. If we want to score the overall feature, we can get the raw explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client = ExplanationClient.from_run(best_run)\n",
    "raw_explanations = client.download_model_explanation(raw=True)\n",
    "raw_explanations.get_feature_importance_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that feature importance scores, we can then [visualize](https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability#visualizations) and interact with them to evaluate the effects of the variables on the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(raw_explanations, fitted_model, X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do the same for a specific run. AML service supports model interpretability and explainability in [several ways](https://docs.microsoft.com/en-us/azure/machine-learning/service/machine-learning-interpretability-explainability#how-to-interpret-your-model). Here we use a direct **Mimic Explainer** with a **LightGBM** as a surrogate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_run, fitted_model = local_run.get_output(iteration = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_explainer_setup_obj = automl_setup_model_explanations(fitted_model, X = X_train, \n",
    "                                                             X_test = X_test, y = y_train,\n",
    "                                                             features = numeric_vars + categorical_vars_E,\n",
    "                                                             task = \"regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = MimicWrapper(ws, automl_explainer_setup_obj.automl_estimator, LGBMExplainableModel, \n",
    "                         init_dataset = automl_explainer_setup_obj.X_transform, run = automl_run,\n",
    "                         features = automl_explainer_setup_obj.engineered_feature_names, \n",
    "                         feature_maps = [automl_explainer_setup_obj.feature_map],\n",
    "                         classes = automl_explainer_setup_obj.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engineered_explanations = explainer.explain(['local', 'global'], eval_dataset = automl_explainer_setup_obj.X_test_transform)\n",
    "engineered_feature_importance = engineered_explanations.get_feature_importance_dict()\n",
    "{k:v for k, v in engineered_feature_importance.items() if v > 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ExplanationDashboard(engineered_explanations, automl_explainer_setup_obj.automl_estimator, automl_explainer_setup_obj.X_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "raw_explanations = explainer.explain(['local', 'global'], get_raw = True, \n",
    "                                     raw_feature_names = automl_explainer_setup_obj.raw_feature_names,\n",
    "                                     eval_dataset = automl_explainer_setup_obj.X_test_transform)\n",
    "raw_explanations.get_feature_importance_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(raw_explanations, automl_explainer_setup_obj.automl_pipeline, automl_explainer_setup_obj.X_test_raw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
